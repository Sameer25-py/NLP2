{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMatMKNXYinz"
   },
   "source": [
    "`Read 'README.TXT'. You need to install packages in requirements.txt first just like assignment 1.` <BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Learning Outcomes:`\n",
    "This notebook will teach you: how sklearn was efficiently implementing tfidf, importance of sparse matrices, some linear algebra, langauge modeling using n-grams, and how cosine similarity can do wonders. We hope you appreciate how simple these ideas are at their core. In fact, `Most great ideas are simple at their core`- [an](https://twitter.com/lexfridman/status/1221493526703366146) amazing quote by your TAs.    \n",
    "There's a lot to learn and what you get out of this is upto you; We have also provided you a supplementary set of notes which you may find useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4O26GfEYin2"
   },
   "source": [
    "# Who's the author?\n",
    "\n",
    "In this problem you will develop a technique for analyzing free text documents: a bag of words approach based on a TFIDF matrix. Here, you'll learn how did sklearn compute TFIDF for you in the other notebook in which you played with tweets. In other words, you'll implement TFIDF without using sklearn.\n",
    "\n",
    "You'll be applying your model to the text from the Federalist Papers.  The Federalist papers were a series of essay written in 1787 and 1788 by Alexander Hamilton, James Madison, and John Jay (they were published anonymously at the time), that promoted the ratification of the U.S. Constitution.  If you're curious, you can read more about them [here](https://en.wikipedia.org/wiki/The_Federalist_Papers). They are a particularly interesting data set, because although the authorship of most of the essays has been long known since around the deaths of Hamilton and Madison, there was still some question about the authorship of certain articles into the 20th century.  You'll use document vectors to do this analysis for yourself. Fundamental concepts like Dot product or [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) that you've been learning since grade 9 will be used here to predict the authorship of the Federalist papers. Awesome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8wll69hYin3"
   },
   "outputs": [],
   "source": [
    "import collections # optional, but we found the collections.Counter object useful\n",
    "import itertools\n",
    "import gzip\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from testing.testing import test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHkrOfH7Yin6"
   },
   "source": [
    "## The dataset\n",
    "\n",
    "You'll use a copy of the Federalist Papers downloaded from Project Guttenberg, available [here]( http://www.gutenberg.org/ebooks/18).  Specifically, the \"pg18.txt.gz\" file included with the homework is the raw text downloaded from Project Guttenberg.  To ensure that everyone starts with the exact same corpus, we are providing you the code to load and tokenize this document, as given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G32sHv27Yin8"
   },
   "outputs": [],
   "source": [
    "def load_federalist_corpus(filename):\n",
    "    \"\"\" Load the federalist papers as a tokenized list of strings\"\"\"\n",
    "    with gzip.open(filename, \"rt\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    papers = data.split(\"FEDERALIST\")\n",
    "    \n",
    "    # all start with \"To the people of the State of New York:\" (sometimes . instead of :)\n",
    "    # all end with PUBLIUS (or no end at all)\n",
    "    locations = [(i,[-1] + [m.end()+1 for m in re.finditer(r\"of the State of New York\", p)],\n",
    "                 [-1] + [m.start() for m in re.finditer(r\"PUBLIUS\", p)]) for i,p in enumerate(papers)]\n",
    "    papers_content = [papers[i][max(loc[1]):max(loc[2])] for i,loc in enumerate(locations)]\n",
    "\n",
    "    # discard entries that are not actually a paper\n",
    "    papers_content = [p for p in papers_content if len(p) > 0]\n",
    "\n",
    "    # replace all whitespace with a single space\n",
    "    papers_content = [re.sub(r\"\\s+\", \" \", p).lower() for p in papers_content]\n",
    "\n",
    "    # add spaces before all punctuation, so they are separate tokens\n",
    "    punctuation = set(re.findall(r\"[^\\w\\s]+\", \" \".join(papers_content))) - {\"-\",\"'\"}\n",
    "    for c in punctuation:\n",
    "        papers_content = [p.replace(c, \" \"+c+\" \") for p in papers_content]\n",
    "    papers_content = [re.sub(r\"\\s+\", \" \", p).lower().strip() for p in papers_content]\n",
    "    \n",
    "    papers_content =[re.sub(r\"[^a-zA-Z0-9]+\",\" \",p) for p in papers_content ]\n",
    "    \n",
    "    authors = [tuple(re.findall(\"MADISON|JAY|HAMILTON\", a)) for a in papers]\n",
    "    authors = [a for a in authors if len(a) > 0]\n",
    "    \n",
    "    numbers = [re.search(r\"No\\. \\d+\", p).group(0) for p in papers if re.search(r\"No\\. \\d+\", p)]\n",
    "    \n",
    "    return papers_content, authors, numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6U9OkgzYin-"
   },
   "outputs": [],
   "source": [
    "docs,authors,numbers=load_federalist_corpus('pg18.txt.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDcN2tJ9YioB"
   },
   "source": [
    "You're welcome to dig through the code here if you're curious, but it's more important that you look at the objects that the function returns.  The `PAPERS` object is a list of strings, each one containing the full content of one of the Federalist Papers.  All tokens (words) in the text are separated by a single space (this includes some puncutation tokens, which have been modified to include spaces both before and after the punctuation. The `AUTHORS` object is a list of lists, which each list contains the author (or potential authors) of a given paper.  Finally the `NUMBERS` list just contains the number of each Federalist paper.  You won't need to use this last one, but you may be curious to compare the results of your textual analysis to the opinion of historians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N32MOZfvYioC"
   },
   "source": [
    "## Bag of words, and TFIDF\n",
    "\n",
    "You'll use a bag of words model to describe the corpus, and write routines to build a TFIDF matrix and a cosine similarity function.  Specifically, you should first implement the TFIDF function below.  This should return a _sparse_ TFIDF matrix (make sure to directly create a sparse matrix using [`scipy.sparse.coo_matrix()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix) rather than create a dense matrix and then convert it to be sparse).\n",
    "\n",
    "You should create the tfidf vector using numpy matrix operations and not use an existing implementation.\n",
    "\n",
    "Important: make sure you do _not_ include the empty token `\"\"` as one of your terms. <br>\n",
    "`You may the supplementary notes of Zico Kolter on matrices (present in notes directory) helpful especially the part on sparse matrices.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTBMz_grYioD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING tfidf: PASSED 9/9\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA = [\"the goal of this lecture is to explain the basics of free text processing\",\n",
    "             \"the bag of words model is one such approach\",\n",
    "             \"text processing via bag of words\"]\n",
    "\n",
    "def tfidf_test(tfidf_impl):\n",
    "    # Simple starting test case:\n",
    "    X_tfidf, words = tfidf_impl([\"A A\", \"A\", \"\"])\n",
    "    test.equal(words, [\"A\"])\n",
    "    test.equal(np.round(X_tfidf.todense().tolist(), 4).tolist(), [[0.8109], [0.4055], [0.]])\n",
    " \n",
    "    # Little more complicated:\n",
    "    X_tfidf, words = tfidf_impl([\"A A A C\", \"A B\", \"C\"])\n",
    "    test.equal(sorted(words), list(\"ABC\"))\n",
    "    # Get word indices in order:\n",
    "    X_tfidf = X_tfidf.todense()\n",
    "    X_lookup = { w: X_tfidf[:,i].ravel() for i, w in enumerate(words)}\n",
    "\n",
    "    # Check A, B, and C columns in order:\n",
    "    test.equal(np.round(X_lookup[\"A\"], 4).tolist(), [[1.2164, 0.4055, 0.    ]])\n",
    "    test.equal(np.round(X_lookup[\"B\"], 4).tolist(), [[0.    , 1.0986, 0.    ]])\n",
    "    test.equal(np.round(X_lookup[\"C\"], 4).tolist(), [[0.4055, 0.    , 0.4055]])\n",
    "    \n",
    "    # With test data from above\n",
    "    X_tfidf, words = tfidf_impl(TEST_DATA)\n",
    "    X_tfidf=X_tfidf.todense()\n",
    "    test.equal(X_tfidf.shape, (3, 19))\n",
    "    test.equal(set(words), {'one', 'bag', 'goal', 'explain', 'approach', 'to', 'processing', 'this', 'the', 'model', 'basics', 'free', 'words', 'such', 'is', 'text', 'lecture', 'via', 'of'})\n",
    "    test.equal(X_tfidf[0, words.index('explain')], 1.0986122886681098)\n",
    "\n",
    "@test\n",
    "def tfidf(docs):\n",
    "    \"\"\"\n",
    "    Create TFIDF matrix.  This function creates a TFIDF matrix from the\n",
    "    docs input.\n",
    "\n",
    "    Args:\n",
    "        docs: list of strings, where each string represents a space-separated\n",
    "              document\n",
    "    \n",
    "    Returns: tuple: (tfidf, all_words)\n",
    "        tfidf: sparse matrix (in any scipy sparse format) of size (# docs) x\n",
    "               (# total unique words), where i,j entry is TFIDF score for \n",
    "               document i and term j\n",
    "        all_words: list of strings, where the ith element indicates the word\n",
    "                   that corresponds to the ith column in the TFIDF matrix\n",
    "    \"\"\"\n",
    "    import math\n",
    "    #tokenizing\n",
    "    \n",
    "    doc_tokens=[]\n",
    "    for i in docs:\n",
    "        i=i.strip()\n",
    "        tokens=i.split(' ')\n",
    "        doc_tokens.append(tokens)\n",
    "        tokens=[]\n",
    "\n",
    "    \n",
    "    u_tokens=[]\n",
    "    #unique tokens\n",
    "    \n",
    "    for doc in docs:\n",
    "        tokens=doc.split(' ')\n",
    "        u_tokens=u_tokens + tokens\n",
    "        u_tokens=list(dict.fromkeys(u_tokens))\n",
    "    try:\n",
    "        u_tokens.remove(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    #df\n",
    "    df_vec=np.zeros((len(u_tokens),),dtype=float)\n",
    "    \n",
    "    for i,word in enumerate(u_tokens):\n",
    "        for j,doc in enumerate(doc_tokens):\n",
    "            if word in doc:\n",
    "                df_vec[i]+=1\n",
    "    for i,val in enumerate(df_vec):\n",
    "        df_vec[i]=math.log(len(doc_tokens)/val)\n",
    "    #tf\n",
    "    tf_matrix=np.zeros((len(u_tokens),len(doc_tokens)),dtype=float)\n",
    "    for i,word in enumerate(u_tokens):\n",
    "        for j,doc in enumerate(doc_tokens):\n",
    "            count=doc.count(word)\n",
    "            tf_matrix[i][j]=count\n",
    "    old_shape=df_vec.shape[0]\n",
    "    df_vec=df_vec.reshape(old_shape,1)\n",
    "    matrix=tf_matrix * df_vec\n",
    "    matrix=np.array(matrix).transpose()\n",
    "    matrix=sp.coo_matrix(matrix)\n",
    "    #print(np.round(matrix.todense().tolist(), 4).tolist())\n",
    "    return matrix,u_tokens\n",
    "\n",
    "matrix,u_tokens=tfidf(docs)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<86x8579 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 57392 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7LN8fVsYioH"
   },
   "source": [
    "Our version results the following result (just showing the type, size, and # of non-zero elements):\n",
    "\n",
    "    <86x8686 sparse matrix of type '<type 'numpy.float64'>'\n",
    "        with 57607 stored elements in Compressed Sparse Row format>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QrvRtLZsYioI"
   },
   "source": [
    "Next, implement the following simple function that takes the X_tfidf matrix (though it could also take simple term frequency matrices, etc), and compute a matrix of all pair-wise cosine similarities.\n",
    "\n",
    "Hints:\n",
    "- cosine similarity is a normalized inner product between the vectors\n",
    "- TFIDF (sparse) contains a vector for each document. First normalize this. You may find [scipy.sparse.linalg.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.norm.html) useful.\n",
    "- `You may find supplementary notes of Zico Kolter (in notes directory) called free_text_notes helpful. Note that in the notes an implmentation using numpy is already given, you've to use the same idea but work with sparse matrices instead.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xorGFPf1YioJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING cosine_similarity: PASSED 2/2\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import norm\n",
    "def cosine_similarity_test(cosine_similarity_impl):\n",
    "     # Little more complicated:\n",
    "    X_tfidf, words = tfidf([\"A A A C\", \"A B\", \"C\"])\n",
    "    M = cosine_similarity_impl(X_tfidf)\n",
    "    if M is None:\n",
    "        return test.true(False)\n",
    "    test.true(np.allclose(M, np.matrix([[1., 0.32847358, 0.31622777], [0.32847358, 1., 0.], [0.31622777, 0., 1.]])))\n",
    "\n",
    "    # Test data\n",
    "    X_tfidf, words = tfidf(TEST_DATA)\n",
    "    M = cosine_similarity_impl(X_tfidf)\n",
    "    test.true(np.allclose(M, \n",
    "        np.matrix([[1., 0.06796739, 0.07771876], [0.06796739, 1., 0.10281225], [0.07771876, 0.10281225, 1.]])))\n",
    "\n",
    "@test\n",
    "def cosine_similarity(X):\n",
    "    \"\"\"\n",
    "    Return a matrix of cosine similarities.\n",
    "    \n",
    "    Args:\n",
    "        X: sparse matrix of TFIDF scores or term frequencies\n",
    "    \n",
    "    Returns:\n",
    "        M: dense numpy array of all pairwise cosine similarities.  That is, the \n",
    "           entry M[i,j], should correspond to the cosine similarity between the \n",
    "           ith and jth rows of X.\n",
    "    \"\"\"\n",
    "    X=X.todense()\n",
    "    X = X / np.linalg.norm(X, axis=1)[:,None]\n",
    "    M = X @ X.T\n",
    "    return M\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kIovw88EYioM"
   },
   "source": [
    "Finally, use this model to analyze potential authorship of the unknown Federalist Papers.  Specifically, compute the average cosine similarity between all the _known_ Hamilton papers and all the _unknown_ papers (and similarly between known Madison and unknown, and Jay and unknown).  Fill out the following function to compute and return these averages.\n",
    "\n",
    "Hints:\n",
    "\n",
    "1. fit a single TFIDF encoding to all papers and transform all papers using it before computing the similarity measure\n",
    "2. for the cosine similarity to be useful when comparing documents, they must all be encoded the same way. Transform all papers together before comparing cosine similarity.\n",
    "3. the unknown papers have author=`(\"HAMILTON\",\"MADISON\")`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iZVyi1qhYioN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING author_cosine_similarity: PASSED 1/1\n",
      "###\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.07018835265325667, 0.09043144766267826, 0.06526542214459052)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def author_cosine_similarity_test(author_cosine_similarity_impl):\n",
    "    papers, authors, numbers = load_federalist_corpus(\"pg18.txt.gz\")\n",
    "    hamilton_mcs, madison_mcs, jay_mcs = author_cosine_similarity_impl(papers, authors)\n",
    "    test.equal(np.round(jay_mcs, 4), 0.0653)\n",
    "\n",
    "@test\n",
    "def author_cosine_similarity(docs, authors):\n",
    "    \"\"\"\n",
    "    Return a tuple of average cosine similarities between all the known papers for a given author\n",
    "    and all the unknown papers.\n",
    "    \n",
    "    Args:\n",
    "        docs: list of strings, where each string represents a space-separated\n",
    "              document\n",
    "        authors: list of lists, which each list contains the author (or potential authors) of a given document\n",
    "    \n",
    "    Returns: tuple: (hamilton_mcs, madison_mcs, jay_mcs)\n",
    "        hamilton_mcs: Average cosine similarity between all the known Hamilton papers and all the unknown papers.\n",
    "        madison_mcs: Average cosine similarity between all the known Madison papers and all the unknown papers.\n",
    "        jay_mcs: Average cosine similarity between all the known Jay papers and all the unknown papers.\n",
    "    \"\"\"\n",
    "    hamilton=[i for i in range(0,len(authors)) if authors[i]==('HAMILTON',)]\n",
    "    madison=[i for i in range(0,len(authors)) if authors[i]==('MADISON',)]\n",
    "    jay=[i for i in range(0,len(authors)) if authors[i]==('JAY',)]\n",
    "    unknown=[i for i in range(0,len(authors)) if authors[i]==('HAMILTON','MADISON')]\n",
    "    matrix,words=tfidf(docs)\n",
    "    sim=cosine_similarity(matrix)\n",
    "    sim=np.array(sim)\n",
    "    a=[]\n",
    "    for i in unknown:\n",
    "        for j in hamilton:\n",
    "            a.append(sim[i][j])\n",
    "    ham_mcs=sum(a)/len(a)\n",
    "    a=[]\n",
    "    \n",
    "    for i in unknown:\n",
    "        for j in madison:\n",
    "            a.append(sim[i][j])\n",
    "    mad_mcs=sum(a)/len(a)\n",
    "    a=[]\n",
    "    \n",
    "    for i in unknown:\n",
    "        for j in jay:\n",
    "            a.append(sim[i][j])\n",
    "    jay_mcs=sum(a)/len(a)\n",
    "    a=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (ham_mcs,mad_mcs,jay_mcs)\n",
    "author_cosine_similarity(docs,authors)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Author_Prediction_Using_cosing_similarity_on_TFIDF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
